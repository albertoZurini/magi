{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxcftRy1Bn6S"
      },
      "source": [
        "# Hands-on Session 1 -- Handling strings with Python - Text Preprocessing - State of the Art Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-8VkCTnCBQL"
      },
      "source": [
        "In this first coding lesson we will look at:\n",
        "- How use python to handle strings.\n",
        "- How to preprocess text (tokenization/lemmatization) for classical NLP applications\n",
        "- The importance of tokenization in modern LLMs and the BPE algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xkzjx1ACnVU"
      },
      "source": [
        "## 1. Basic of Python Strings\n",
        "\n",
        "### 1.1 What is a string?\n",
        "In Python, a string is an ordered sequence of characters used to represent text. Strings are a fundamental data type and are essential for working with text data in natural language processing.\n",
        "\n",
        "Characteristics of String in Python:\n",
        "- __immutable__: Once a string is created, it cannot be modified.\n",
        "- __Sequence Type__: Strings are sequences (of characters), so we can use indexing and slices.\n",
        "- __Unicode Support__: Python 3 use Unicode by defualt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrSa-yjR6U01",
        "outputId": "d4015482-76b1-4ee4-e4a6-de6ee572132f"
      },
      "outputs": [],
      "source": [
        "#define a string\n",
        "my_string = \"Hello, NLP World!\"\n",
        "\n",
        "my_string_multiline = \"\"\" This s a multi\n",
        "line\n",
        "string\n",
        "\"\"\"\n",
        "\n",
        "#print the string\n",
        "print(my_string)\n",
        "print(my_string_multiline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz5OEGhLEV_Y",
        "outputId": "6e6bfea2-6336-43af-a29b-6dda3cc5ac2f"
      },
      "outputs": [],
      "source": [
        "for char in my_string:\n",
        "    print(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "kEfCd-kDEw-G",
        "outputId": "370e45c4-a575-4c8e-e889-0d1ebe60e44a"
      },
      "outputs": [],
      "source": [
        "my_string[2] = \"X\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEvXJBDXE6cQ"
      },
      "source": [
        "#### 1.1.1. Unicode and UTF-8\n",
        "\n",
        "__What is Unicode?__\n",
        "\n",
        "Unicode is a universal character encoding standard that assigns a unique code point to every character from virtually all writing systems, symbols, and emojis.\n",
        "It enables consistent representation and manipulation of text across different platforms and programs.\n",
        "Unicode supports over 143,000 characters, covering scripts like Latin, Greek, Cyrillic, Arabic, Chinese, and more.\n",
        "\n",
        "__UTF-8 Encoding__\n",
        "\n",
        "UTF-8 (Unicode Transformation Format - 8-bit) is a variable-length character encoding for Unicode.\n",
        "It encodes each Unicode character into one to four bytes:\n",
        "- 1 byte for standard ASCII characters (U+0000 to U+007F).\n",
        "- 2 to 4 bytes for characters outside the ASCII range.\n",
        "- Advantages of UTF-8:\n",
        " - Backward Compatibility: UTF-8 is compatible with ASCII, making it ideal for systems originally designed for ASCII.\n",
        "- Efficiency: It uses fewer bytes for common characters, which is space-efficient for texts dominated by ASCII characters.\n",
        "- Flexibility: Can represent any Unicode character, accommodating multiple languages and symbols.\n",
        "\n",
        "***References***:\n",
        "\n",
        "1. [Unicode](https://home.unicode.org/)\n",
        "2. [UTF-8 Wikipedia](https://it.wikipedia.org/wiki/UTF-8)\n",
        "3. [UTF-8 Everywhere](https://utf8everywhere.org/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHeSFXg3GAyt",
        "outputId": "899c7628-10bf-46e0-a8d5-27d0edd6714d"
      },
      "outputs": [],
      "source": [
        "# Convert a string to UTF-8\n",
        "\n",
        "my_string = \"Hello, NLP World! ðŸš€ ä½ å¥½ä¸–ç•Œ \"\n",
        "\n",
        "encoded_string = my_string.encode(\"UTF-8\")\n",
        "\n",
        "print(\"Encoded string:\", encoded_string)\n",
        "\n",
        "encoded_string_integer = list(encoded_string)\n",
        "\n",
        "print(\"Encoded string integer:\", encoded_string_integer)\n",
        "\n",
        "decoded_string = encoded_string.decode(\"UTF-8\")\n",
        "\n",
        "print(\"Decoded string:\", decoded_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj7_MNCEG1iK"
      },
      "source": [
        "### 1.2. Handling strings in Python\n",
        "Python provides a rich set of built-in functions and methods for working with strings. Here are some common operations:\n",
        "\n",
        "#### String Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmv4YAUBG8jf"
      },
      "outputs": [],
      "source": [
        "# f-string\n",
        "x = {\n",
        "    \"name\": \"John\",\n",
        "    \"age\": 30\n",
        "}\n",
        "\n",
        "print(f\"My name is {x['name']} and I am {x['age']} years old.\")\n",
        "\n",
        "# format\n",
        "print(\"My name is {} and I am {} years old.\".format(x['name'], x['age']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkjBf8fCIEaV"
      },
      "source": [
        "#### String manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFcstVpIH25L",
        "outputId": "1ca3229b-923e-4f4f-c8a8-4f6c45ce8849"
      },
      "outputs": [],
      "source": [
        "text_1 = \" Trieste is a beautiful city near the border with Croatia and Slovenia. \"\n",
        "text_2 = \"\"\"Trieste is famous for its cafes and historic architecture.\n",
        "\"\"\"\n",
        "\n",
        "# SUm string\n",
        "text = text_1 + text_2\n",
        "\n",
        "print(text)\n",
        "\n",
        "\n",
        "print(text.upper())\n",
        "print(text.lower())\n",
        "print(text.title())\n",
        "\n",
        "# Remove Puntuaction\n",
        "import string\n",
        "print(text.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "# Remove white spaces\n",
        "print(text.strip())\n",
        "\n",
        "# Remove left with space\n",
        "print(text.lstrip())\n",
        "\n",
        "# Remove right with space\n",
        "print(text.rstrip())\n",
        "\n",
        "# Replace\n",
        "\n",
        "print(text.replace(\"Trieste\", \"Milano\"))\n",
        "\n",
        "# Find\n",
        "print(text.find(\"Croatia\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VONpZVbNIkLj",
        "outputId": "860aba4b-252c-4db9-95b6-d1917918d835"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Split\n",
        "print(text.split())\n",
        "\n",
        "print(text.split(\".\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwXq72HKcMmh"
      },
      "outputs": [],
      "source": [
        "text = \"Is a student of NLP in Trieste.\"\n",
        "\n",
        "names = [\"jhon\", \"mary\"]\n",
        "surnames = [\"doe\", \"lou\"]\n",
        "cities = [\"Trieste\", \"Milano\"]\n",
        "\n",
        "# Exercise: Manipulate the three strings in order to print, for each triple of (Name, Surname, City) plot the string \"Name Surname is a student of NLP in CITY\". For example \"Jhon Doe is a student of NLP in MILANO\"\n",
        "# DO NOT declare new string object, but just compose and manipulate the previous strings\n",
        "\n",
        "for name, surname, city in zip(names, surnames, cities):\n",
        "    print(f\"{name.capitalize()} {surname.capitalize()} is a student of NLP in {city.upper()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYcMrZXGliz3"
      },
      "source": [
        "### 1.3 Regular Expressions in Python\n",
        "\n",
        "Regular expressions (regex) are sequences of characters that define a search pattern. They are widely used for string matching and manipulation.\n",
        "\n",
        "In Python, the `re` module provides support for regular expressions.\n",
        "\n",
        "#### Basics of Regular Expressions\n",
        "\n",
        "- **Literal Characters**: Matches the exact character.\n",
        "- **Metacharacters**: Characters with special meaning.\n",
        "\n",
        "Some common metacharacters:\n",
        "\n",
        "- `.` : Matches any character except a newline.\n",
        "- `^` : Matches the start of a string.\n",
        "- `$` : Matches the end of a string.\n",
        "- `*` : Matches 0 or more repetitions.\n",
        "- `+` : Matches 1 or more repetitions.\n",
        "- `?` : Matches 0 or 1 repetition.\n",
        "- `[]`: Matches any one character inside the brackets.\n",
        "- `|` : Matches either the expression before or the expression after the `|`.\n",
        "- `\\d`: Matches any decimal digit; equivalent to [0-9].\n",
        "- `\\D`: Matches any non-digit character.\n",
        "- `\\s`: Matches any whitespace character.\n",
        "- `\\S`: Matches any non-whitespace character.\n",
        "- `\\w`: Matches any alphanumeric character and underscore.\n",
        "- `\\W`: Matches any non-alphanumeric character.\n",
        "\n",
        "Let's start with some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CHwucc1dTPw"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "# Simple pattern matching\n",
        "pattern = r\"apple\"\n",
        "text = \"I like apples and apple pies.\"\n",
        "\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQGYg1MAlnxc"
      },
      "source": [
        "The `re.findall()` function returns all non-overlapping matches of the pattern in the string, as a list of strings.\n",
        "\n",
        "#### Using Metacharacters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIVNNr3Nlp9b"
      },
      "outputs": [],
      "source": [
        "# Finding all digits\n",
        "text = \"My phone number is 123-456-7890.\"\n",
        "\n",
        "pattern = r\"\\d\"\n",
        "digits = re.findall(pattern, text)\n",
        "print(digits)\n",
        "\n",
        "# Finding all sequences of digits\n",
        "pattern = r\"\\d+\"\n",
        "digits_sequences = re.findall(pattern, text)\n",
        "print(digits_sequences)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYoX1E3blqpM"
      },
      "source": [
        "#### re.search() vs re.match()\n",
        "\n",
        "- `re.match()` checks for a match only at the beginning of the string.\n",
        "- `re.search()` checks for a match anywhere in the string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqT2Am7alvEN"
      },
      "outputs": [],
      "source": [
        "\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Using re.match()\n",
        "match = re.match(r'cat', text)\n",
        "print(\"Using re.match():\", match)\n",
        "\n",
        "# Using re.search()\n",
        "search = re.search(r'cat', text)\n",
        "print(\"Using re.search():\", search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qixEq8orlxU1"
      },
      "source": [
        "#### re.sub()\n",
        "\n",
        "The `re.sub()` function replaces occurrences of the pattern with a specified replacement string.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHPaE9Tfly4n"
      },
      "outputs": [],
      "source": [
        "text = \"I have a cat. My cat is cute.\"\n",
        "\n",
        "# Replace 'cat' with 'dog'\n",
        "new_text = re.sub(r'cat', 'dog', text)\n",
        "print(new_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIdu_NhsmR62"
      },
      "source": [
        "#### Grouping and Capturing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvbBP0e3mTMu"
      },
      "outputs": [],
      "source": [
        "text = \"My email is john.doe@example.com\"\n",
        "\n",
        "# Pattern to extract email\n",
        "pattern = r'(\\w+.\\w+)@(\\w+\\.\\w+)'\n",
        "\n",
        "match = re.search(pattern, text)\n",
        "if match:\n",
        "    print(\"Full match:\", match.group(0))\n",
        "    print(\"Username:\", match.group(1))\n",
        "    print(\"Domain:\", match.group(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvR2wtoNl8Lm"
      },
      "source": [
        "\n",
        "#### Compiling Regular Expressions\n",
        "\n",
        "For patterns that will be used multiple times, it's more efficient to compile them.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lgpY472l6Wu"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r'\\d+')\n",
        "\n",
        "text1 = \"Order number 12345\"\n",
        "text2 = \"Invoice 67890\"\n",
        "\n",
        "print(pattern.findall(text1))\n",
        "print(pattern.findall(text2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zv2GKCdl-DN"
      },
      "source": [
        "#### Exercise: Regular Expressions\n",
        "\n",
        "**Task**: Write a regular expression to extract all valid email addresses from the following text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21kNxiSlmBWd"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Please contact us at support@example.com for further information.\n",
        "You can also reach out to sales@example.co.uk or feedback@company.org.\n",
        "Invalid emails like test@.com or @example.com should not be matched.\n",
        "\"\"\"\n",
        "\n",
        "# Write your code here\n",
        "pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
        "\n",
        "emails = re.findall(pattern, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Extracted emails:\", emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80O1KOD4j-Vv"
      },
      "source": [
        "## 2 - Tokeniziation and Lemmatization\n",
        "Tokenization and lemmatization are fundamental steps in text preprocessing for Natural Language Processing (NLP).\n",
        "\n",
        "- **Tokenization**: The process of breaking text into smaller units called tokens (e.g., words, sentences).\n",
        "- **Lemmatization**: Reducing words to their base or dictionary form (lemma).\n",
        "\n",
        "We will use the Natural Language Toolkit (NLTK), a popular Python library for NLP tasks.\n",
        "\n",
        "Why Tokenization?\n",
        "- Text data is unstructured and needs to be converted into a structured format for analysis. Tokenization breaks text into smaller units (tokens) for further processing.\n",
        "- Tokens can be words, sentences, or subwords, depending on the task.\n",
        "\n",
        "Problem of Tokenization:\n",
        "- Tokenization is not always straightforward due to the complexity of languages and text data.\n",
        "- \"New York\" can be considered as one token or two tokens?\n",
        "- \"can't\" can be split into \"can\" and \"not\" or kept as a single token?\n",
        "- Tokenization depends on the context and the task at hand.\n",
        "- How to handle new words or out-of-vocabulary (OOV) terms?\n",
        "\n",
        "### 2.1 Tokenization using NLTK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NpOdEMMmfIP"
      },
      "outputs": [],
      "source": [
        "# Install NLTK if not already installed\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh6QtDXBmmeG"
      },
      "source": [
        "#### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J4XINVv1mm0L"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello! How are you doing today? It's great to see you.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWICpVpmpuB"
      },
      "source": [
        "#### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNK0aBdDmqOv"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello! How are you doing today? It's great to see you. Let's catch up soon.\"\n",
        "\n",
        "sentences = sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZu_fNHOms6x"
      },
      "source": [
        "### 2.2 Lemmatization using NLTK\n",
        "\n",
        "Lemmatization requires the use of a dictionary to find the lemma of a word.\n",
        "\n",
        "We need to download the WordNet lemmatizer and the POS (Part of Speech) tagger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKNj5MgZmv2Z"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX2jmAEfnKe4"
      },
      "source": [
        "To improve lemmatization, we need to provide the correct POS tag for each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfsufIVonULC"
      },
      "outputs": [],
      "source": [
        "text = \"The striped bats are hanging on their feet for best. \"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "lemmatized_output = [lemmatizer.lemmatize(word) for word in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print([f\"{word} -> {lemmatizer.lemmatize(word)}\" for word in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Stemming using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"The striped bats are hanging on their feet for best. \"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "stemmed_output = [stemmer.stem(word) for word in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print([f\"{word} -> {stemmer.stem(word)}\" for word in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5aekS9JnWBa"
      },
      "source": [
        "### 2.4 Stopwords Removal\n",
        "\n",
        "Stopwords are common words that may not carry significant meaning (e.g., 'is', 'the', 'and').\n",
        "\n",
        "We can remove them to reduce noise in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYgqcG6YnXUz"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 - Basic Sentiment Analysis using Vocabulary-based Approach\n",
        "\n",
        "Sentiment analysis is the process of determining the sentiment (positive, negative, neutral) of a text. We can use a vocabulary-based approach to classify text based on the presence of positive or negative words.\n",
        "The process involves the following steps:\n",
        "- Load a list of positive and negative words.\n",
        "- Tokenize the text.\n",
        "- Count the number of positive and negative words.\n",
        "- Determine the overall sentiment based on the counts.\n",
        "- Calculate the sentiment score.\n",
        "- Classify the sentiment as positive, negative, or neutral.\n",
        "- \n",
        "Let's implement a basic sentiment analysis using this approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Brief intro to ðŸ¤— datasets\n",
        "ðŸ¤— datasets offers a pactical way to handle and share datasets. It cames as a GitHub of datasets. Given a dataset, you can see the repository on the web, for example [imdb](https://huggingface.co/datasets/stanfordnlp/imdb). Let's explore the basics functionalities of the library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# Load a dataset\n",
        "# For this example, we will use the 'imdb' dataset\n",
        "dataset = load_dataset('imdb')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the dataset structure\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Explore the dataset\n",
        "# Display the first few examples from the training set\n",
        "print(\"Training set examples:\")\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle the dataset\n",
        "dataset = dataset.shuffle()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the dataset\n",
        "# Define a preprocessing function to lowercase the text\n",
        "def preprocess_function(examples):\n",
        "    return {'text': [text.upper() for text in examples['text']]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the dataset\n",
        "dataset = dataset.map(preprocess_function, batched=True, num_proc=4)\n",
        "\n",
        "# Display the first few preprocessed examples\n",
        "print(\"\\nPreprocessed training set examples:\")\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load a negative and a positive word list\n",
        "file = open(\"wordwithStrength.txt\").read().splitlines()\n",
        "\n",
        "# EXERCISE: Create a dictionary with the words as keys and the weights as values\n",
        "WORDS_WITH_WEIGHT = {}\n",
        "for word in file:\n",
        "    WORDS_WITH_WEIGHT[word.split(\"\\t\")[0]] = float(word.split(\"\\t\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the words with their weights\n",
        "print(WORDS_WITH_WEIGHT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load a sentiment analysis dataset\n",
        "from datasets import load_dataset\n",
        "from typing import List\n",
        "dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "\n",
        "#shuffle the dataset\n",
        "import random\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# sample just 25 reviews\n",
        "dataset = dataset.select(range(25))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_review(review: str) -> List:\n",
        "    # Tokenize a review into words\n",
        "    tokenized_review = word_tokenize(review)\n",
        "\n",
        "    # to lowercase\n",
        "    tokenized_review = [word.lower() for word in tokenized_review]\n",
        "    \n",
        "    return tokenized_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_review(review: List):\n",
        "    # count the number of positive and negative words in a review\n",
        "\n",
        "    balance = 0\n",
        "    n_found = 0\n",
        "    for word in review:\n",
        "        if word in WORDS_WITH_WEIGHT:\n",
        "            balance += WORDS_WITH_WEIGHT[word]\n",
        "            n_found += 1\n",
        "    return balance / n_found if n_found > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for review in dataset:\n",
        "    review_text = review[\"text\"]\n",
        "    true_label = review[\"label\"]\n",
        "    tokenized_review = tokenize_review(review_text)\n",
        "    balance = count_review(tokenized_review)\n",
        "    \n",
        "    print(\"Review:\", review)\n",
        "    print(\"Balance:\", balance, \"True Label:\", true_label)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RLSZKbVXEG7"
      },
      "source": [
        "## 4 - Modern Tokenization, aka what we use in LLM nowdays?\n",
        "\n",
        "The tokenization algorithm currently employed in LLMs try to create a trade-off between the following property:\n",
        "- Vocabulary size is larger enough to contain (some) semantic information\n",
        "- Vocabulary size is small enough to be handled.\n",
        "- (Bonus) How can we handle out-of-vocabulary words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSdlQ9AHZG2e"
      },
      "source": [
        "__Why Do We Need a Large Vocabulary?__\n",
        "\n",
        "Imagine if we encoded all text using only the 26 letters of the alphabet. Tokenization would be straightforward, but the resulting sequences would be very long. This leads to two main issues:\n",
        "\n",
        "- __Increased computational cost__: Longer sequences require more computational resources. If you recall from the Deep Learning course, the attention mechanism in transformers has a complexity that scales quadratically with the input length. This means that longer sequences can significantly increase the computation needed, making the process less efficient.\n",
        "\n",
        "- __Loss of information__: With longer sequences, the information gets spread out, making it harder for machine learning models, especially transformers, to learn. Instead of directly recognizing and understanding words, the model would first have to learn how to construct words from individual letters before it can start learning the relationships between words. This adds an unnecessary layer of complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Of1honpZGSI",
        "outputId": "b5857f9a-c0a0-435e-90a9-d7c532a6572e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example sentence\n",
        "text = \"The quick brown fox jumps over the lazy dog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Character-level tokenization (very small vocabulary)\n",
        "char_tokens = list(text.replace(\" \", \"\"))\n",
        "char_token_count = len(char_tokens)\n",
        "char_attention_size = char_token_count ** 2\n",
        "\n",
        "print(f\"Original text: {text}\")\n",
        "print(f\"\\nCharacter-level tokenization:\")\n",
        "print(f\"Tokens: {char_tokens}\")\n",
        "print(f\"Number of tokens: {char_token_count}\")\n",
        "print(f\"Attention matrix size: {char_attention_size} elements\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Word-level tokenization (larger vocabulary)\n",
        "word_tokens = text.split()\n",
        "word_token_count = len(word_tokens)\n",
        "\n",
        "# Size of attention matrix: (sequence length) x (sequence length)\n",
        "word_attention_size = word_token_count ** 2\n",
        "\n",
        "print(f\"\\nWord-level tokenization:\")\n",
        "print(f\"Tokens: {word_tokens}\")\n",
        "print(f\"Number of tokens: {word_token_count}\")\n",
        "print(f\"Attention matrix size: {word_attention_size} elements\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjDF89WWa_Vl"
      },
      "source": [
        "__Why don't we use Word-level tokenization instead?__\n",
        "\n",
        "Using word-level tokenization can seem like a good solution because it reduces the length of the tokenized sequence, leading to smaller and more efficient attention matrices. However, there are several challenges with this approach:\n",
        "\n",
        "- Large Vocabulary Size: Word-level tokenization requires a vocabulary that contains every possible word in the language. Since languages are constantly evolving, new words, slang, and domain-specific terms are regularly added, making it impossible to maintain a comprehensive vocabulary. A large vocabulary also means that the model will need to handle a very large set of tokens, increasing memory usage and making the model harder to train.\n",
        "\n",
        "- Out-of-Vocabulary (OOV) Problem: No matter how big the vocabulary is, there will always be words that are not included. For example, rare words, typos, or new terminology might not be part of the predefined set. In such cases, word-level tokenizers often fail because they cannot handle these \"unknown\" words, resulting in a loss of information.\n",
        "\n",
        "- Difficulty Handling Morphology: In many languages, words can change forms depending on grammar rules (e.g., plurals, verb conjugations). A word-level tokenizer would need to include every possible variation (like \"run,\" \"runs,\" \"running,\" etc.) as separate tokens. This increases the vocabulary size and makes it harder for the model to generalize because it sees different forms of the same word as completely separate entities.\n",
        "\n",
        "- Inefficient Handling of Subwords: Some words are made up of common prefixes, suffixes, or roots (like \"unhappiness\" = \"un\" + \"happiness\"). Word-level tokenizers would treat each of these as a separate word, missing the opportunity to learn useful patterns.\n",
        "\n",
        "### Solution: Sub-word tokenizer and Byte-Pair Econding algorithm\n",
        "o address the challenges of character-level and word-level tokenization, we use sub-word tokenizers. These tokenizers aim to find a balance between the two approaches by breaking down words into smaller, meaningful units (sub-words) rather than relying on full words or individual characters. One of the most popular sub-word tokenization techniques is the Byte-Pair Encoding (BPE) algorithm.\n",
        "\n",
        "__What is Byte-Pair Encoding (BPE)?__\n",
        "\n",
        "BPE is a compression-based algorithm that starts by treating each character as a separate token and iteratively merges the most frequently occurring pairs of tokens to form new sub-words. This process continues until a predefined vocabulary size is reached. The result is a set of tokens that can effectively represent common sub-words, prefixes, suffixes, and even complete words.\n",
        "It was popularized by the [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n",
        "\n",
        "__Why BPE Works Well__\n",
        "\n",
        "- Efficient Vocabulary Size: BPE creates a more manageable vocabulary by allowing the model to represent both frequent words as single tokens and less common words as a combination of sub-word tokens. This strikes a balance between character-level and word-level tokenization, resulting in a smaller, more efficient vocabulary without sacrificing flexibility.\n",
        "\n",
        "- Handling Rare and New Words: Since BPE breaks down words into smaller parts, it can effectively handle rare words, new terminology, and even misspellings. If a word is not in the vocabulary, BPE can still encode it by combining smaller sub-word tokens. This helps the model process unseen words without failing, unlike traditional word-level tokenizers.\n",
        "\n",
        "- Learning Morphological Patterns: By segmenting words into sub-words, BPE enables the model to learn useful patterns, such as prefixes, suffixes, and root words. For example, \"running,\" \"runner,\" and \"runs\" might all share the common sub-word \"run,\" making it easier for the model to understand relationships between these forms.\n",
        "\n",
        "- Optimized Computation: Sub-word tokenization reduces the input sequence length compared to character-level tokenization, leading to smaller attention matrices. This makes the process more efficient, reducing computational cost without losing important information.\n",
        "\n",
        "__Example: How BPE Works__\n",
        "\n",
        "Suppose we have the following text:\n",
        "\n",
        "\"low\", \"lowest\", \"lower\"\n",
        "Start by treating each character as a token:\n",
        "\n",
        "\"l\", \"o\", \"w\", \"e\", \"s\", \"t\", \"r\"\n",
        "Count the most frequent pairs:\n",
        "\n",
        "(\"l\", \"o\"), (\"o\", \"w\"), (\"l\", \"o\"), (\"o\", \"w\"), (\"o\", \"w\")\n",
        "Merge the most frequent pair (\"o\", \"w\") into a new token:\n",
        "\n",
        "\"low\", \"l\", \"o\", \"w\", \"e\", \"s\", \"t\"\n",
        "Repeat the process until the vocabulary size is reached:\n",
        "\n",
        "Merge (\"l\", \"o\") -> \"low\", \"lowest\", \"low\", \"e\", \"s\", \"t\"\n",
        "BPE ensures that common words like \"low\" are encoded efficiently as a single token, while more complex or rare words can be built from smaller sub-word pieces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xupXP7DWca0j",
        "outputId": "84348e16-a40e-4b5c-a4d3-ab08ddb6a996"
      },
      "outputs": [],
      "source": [
        "text = \"banana bandana\"\n",
        "tokens = list(text.encode(\"utf-8\"))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_pWllUEcjhe",
        "outputId": "3290f712-529f-4371-b6d8-3d10f24d7377"
      },
      "outputs": [],
      "source": [
        "def count_pair(tokens):\n",
        "    pass\n",
        "print(count_pair(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqLhrOb8ctii"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union, Dict\n",
        "\n",
        "class BPE():\n",
        "    def __init__(self, vocab_size = 260):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.merge_forest = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def string_to_bytes(text: str) -> List[int]:\n",
        "        raise NotImplementedError\n",
        "    @staticmethod\n",
        "    def bytes_to_string(tokens: List[int]) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def count_pair(self, tokens) -> Dict:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def merge(self, pair, tokens: List[int], new_id: id) -> List[int]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def train(self, train_text: Union[List[int], str]):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode(self, text: Union[List[int], str]) -> List[int]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, tokens: List[int], return_type = \"string\") -> Union[str, List[int]]:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcLR0ct0cvrp"
      },
      "outputs": [],
      "source": [
        "bpe = BPE(vocab_size=780)\n",
        "\n",
        "lorem_ipsum = \"\"\"\n",
        "\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas commodo velit non ligula consequat molestie. Pellentesque dui massa, viverra sed quam vitae, maximus mattis leo. Morbi rhoncus sodales convallis. Etiam fermentum dui ex. Ut posuere rutrum lectus in vehicula. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Nullam non metus lobortis, efficitur eros tempus, placerat ligula.\n",
        "\n",
        "Praesent in aliquam odio. Nulla varius sagittis ipsum. Cras lacinia tincidunt nisl, id interdum purus dapibus at. Vestibulum id justo gravida, rutrum tortor at, commodo orci. Sed molestie bibendum tortor, eget ultricies metus posuere sed. Donec venenatis felis massa, in fermentum libero volutpat et. Praesent accumsan consequat ligula at viverra. Proin sagittis dolor quis justo finibus pharetra. In hac habitasse platea dictumst. Mauris in vehicula augue. Duis scelerisque elementum mollis. Vestibulum auctor feugiat egestas. Duis luctus ornare pellentesque. Vestibulum et magna elementum, maximus justo et, feugiat nunc.\n",
        "\n",
        "Ut mattis nec elit vitae placerat. Aenean in eleifend justo. Sed nec molestie felis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur quis ipsum in odio consectetur egestas. Curabitur convallis vulputate eleifend. Vivamus mi mauris, facilisis non placerat efficitur, ornare ac purus. Nullam ornare purus vel dictum pulvinar.\n",
        "\n",
        "Cras lacinia velit et nisi varius, id aliquet ligula posuere. In lacinia, nisi non dictum luctus, metus tortor viverra felis, nec rhoncus velit massa gravida tortor. Morbi purus metus, lobortis et arcu eu, molestie pulvinar mauris. Curabitur condimentum vehicula tempus. Nulla facilisi. Morbi mauris nisl, euismod id risus id, rutrum suscipit quam. Proin in lectus quis turpis ornare feugiat. Integer venenatis dui sem, at fermentum nulla varius vel. Ut tristique scelerisque nisl ut mollis. Cras facilisis, nunc quis tristique elementum, orci odio rhoncus nisl, vitae scelerisque odio ante ac elit.\n",
        "\n",
        "Phasellus nec velit tellus. In tellus dui, euismod ac venenatis sit amet, porttitor id lectus. Sed et mauris at tellus vehicula commodo sed non metus. Donec non dui sit amet neque pretium convallis id eu nibh. Vivamus pharetra ligula eros. Maecenas interdum nibh nec venenatis viverra. Maecenas venenatis convallis est ac tristique. Aliquam erat volutpat. Mauris at velit sed lorem finibus pellentesque. Nam eget ante vel tortor finibus pellentesque. Phasellus bibendum venenatis mi eget molestie. Integer quis tortor at augue scelerisque tincidunt eu id mi. Praesent congue consectetur nulla. Proin est erat, tempus eu sem sit amet, sodales feugiat nibh. Nam at consequat ante. Aliquam fringilla tellus non odio pulvinar tincidunt.\n",
        "\"\"\"\n",
        "\n",
        "bpe.train(train_text=lorem_ipsum)\n",
        "\n",
        "\n",
        "tokens_ids = bpe.encode(\"\"\"Nulla varius sagittis ipsum. Cras lacinia tincidunt nisl, id interdum purus dapibus at. Vestibulum id justo gravida, rutrum tortor at, commodo orci. Sed molestie bibendum tortor, eget ultricies metus posuere sed. Donec venenatis felis massa, in fermentum libero volutpat et. Praesent accumsan consequat ligula at viverra. Proin sagittis dolor quis justo finibus pharetra. In hac habitasse platea dictumst. Mauris in vehicula augue. Duis scelerisque elementum mollis. Vestibulum auctor feugiat egestas. Duis luctus ornare pellentesque. Vestibulum et magna elementum, maximus justo et, feugiat nunc.\n",
        "\n",
        "Ut mattis nec elit vitae placerat. Aenean in eleifend justo. Sed nec molestie felis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur quis ipsum in odio consectetur egestas. Curabitur convallis vulputate eleifend. Vivamus mi mauris, facilisis non placerat efficitur, ornare ac purus. Nullam ornare purus vel dictum pulvinar.\n",
        "\n",
        "Cras lacinia velit et nisi varius, id aliquet ligula posuere. In lacinia, nisi non dictum luctus, metus tortor viverra felis, nec rhoncus velit massa gravida tortor. Morbi purus metus, lobortis et arcu eu, molestie pulvinar mauris. Curabitur condimentum vehicula tempus. Nulla facilisi. Morbi mauris nisl, euismod id risus id, rutrum suscipit quam. Proin in lectus quis turpis ornare feugiat. Integer venenatis dui sem, at fermentum nulla varius vel. Ut tristique scelerisque nisl ut mollis. Cras facilisis, nunc quis tristique elementum, orci odio rhoncus nisl, vitae scelerisque odio ante ac elit.\n",
        "\"\"\")\n",
        "print(tokens_ids)\n",
        "original_text = bpe.decode(tokens_ids)\n",
        "print(original_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bonus\n",
        "- Bonus 1. This implementation does not handle the case of unknown sequence of bytes. Since utf-8 is a variable-length encoding, it is possible that a sequence of bytes is not a valid utf-8 character. In this case, the algorithm will raise an exception. To fix this, implement a try-except block to handle the exception and continue the loop.\n",
        "- Bonus 2. The current implementation do not handle special tokens, like <start_sentence> or <end_sentence>. Modify the algorithm to handle special tokens by adding them to the vocabulary and updating the encoding process accordingly.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtjZEv8adiIZ"
      },
      "source": [
        "## Tokenizer HuggingFace\n",
        "Now let's see how we use in practice pre-trained tokenizer with the HuggingFace transformers library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8ixMDjqdvwe",
        "outputId": "94bc8b4e-10ef-4a2d-dc56-c23c85a86513"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokens = tokenizer.encode(lorem_ipsum)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "print(tokenizer.decode(tokens))\n",
        "\n",
        "print(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeQEZMfSnrVZ"
      },
      "source": [
        "### Additional Resources\n",
        "\n",
        "- [BPE Tokenizer Youtube Video by Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE&t=2155s)\n",
        "- [Regular Expressions Documentation](https://docs.python.org/3/library/re.html)\n",
        "- [HuggingFace Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
        "- [NLTK Book](http://www.nltk.org/book/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_BF0EwdsIG0"
      },
      "source": [
        "## Final Exercise: Building a Text Processing Pipeline for Log File Analysis\n",
        "\n",
        "Objective:\n",
        "Create a comprehensive text processing pipeline to analyze and extract meaningful information from a complex log file. The pipeline will involve:\n",
        "1. Parsing and Cleaning the Log File\n",
        "2. Feature Extraction using Regular Expressions\n",
        "3. Tokenization and Lemmatization\n",
        "4. Custom Byte-Pair Encoding (BPE) Tokenization with Special Tokens\n",
        "5. Data Visualization and Analysis\n",
        "\n",
        "Instructions:\n",
        "Follow the steps outlined in the comments to complete the exercise.\n",
        "\n",
        "---\n",
        "\n",
        "Background:\n",
        "Log files are essential in monitoring and diagnosing systems and applications. They often contain a mix of timestamps, error messages, user actions, and other system-generated information. Analyzing log files can be challenging due to their unstructured and noisy nature.\n",
        "\n",
        "---\n",
        "\n",
        "Deliverables:\n",
        "- Complete all sections marked as TODO.\n",
        "- Ensure your code is well-documented with comments explaining your logic.\n",
        "- Include visualizations with appropriate titles and labels.\n",
        "- Summarize your findings and discuss any challenges faced.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "NAf-EUB7sD22",
        "outputId": "af6a3cf7-93e7-4922-e9d4-9cb182f43814"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "\n",
        "# Step 1: Obtain and Explore the Log File\n",
        "\n",
        "# TODO:\n",
        "# - Load the log file 'system_logs.txt' into Python.\n",
        "# - Handle any encoding issues.\n",
        "# - Read the file line by line for processing.\n",
        "\n",
        "# Hints:\n",
        "# - Use the open() function with the appropriate encoding.\n",
        "# - Read lines using file.readlines().\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# Step 2: Parsing and Cleaning the Logs\n",
        "\n",
        "# TODO:\n",
        "# - Strip unnecessary whitespace from each line.\n",
        "# - Parse each log entry into its components:\n",
        "#   - Timestamp\n",
        "#   - Log Level (INFO, WARNING, ERROR)\n",
        "#   - UserID\n",
        "#   - IP Address\n",
        "#   - Action Message\n",
        "# - Handle any malformed entries.\n",
        "\n",
        "# Hints:\n",
        "# - Use string methods like strip().\n",
        "# - Use exception handling to manage parsing errors.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# Step 3: Feature Extraction using Regular Expressions\n",
        "\n",
        "# TODO:\n",
        "# - Define regex patterns for each component.\n",
        "# - Extract features using the patterns.\n",
        "# - Store the extracted data in a structured format.\n",
        "\n",
        "# Hints:\n",
        "# - Use re.search() or re.match() to apply regex patterns.\n",
        "# - Store data in a list of dictionaries or a pandas DataFrame.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# Step 4: Tokenization and Lemmatization\n",
        "\n",
        "# Ensure required NLTK data packages are downloaded\n",
        "# Uncomment the lines below if running for the first time\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# TODO:\n",
        "# - Prepare the 'action' messages for text processing.\n",
        "# - Tokenize the text into words.\n",
        "# - Remove stopwords and punctuation.\n",
        "# - Perform lemmatization with correct POS tags.\n",
        "# - Add the processed text back to your data structure.\n",
        "\n",
        "# Hints:\n",
        "# - Use word_tokenize() for tokenization.\n",
        "# - Use stopwords.words('english') for stopwords.\n",
        "# - Define a function to map POS tags for lemmatization.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# Step 5: Custom Byte-Pair Encoding (BPE) Tokenization with Special Tokens\n",
        "\n",
        "# TODO:\n",
        "# - Implement or import your BPE class.\n",
        "# - Modify the BPE tokenizer to accept special tokens.\n",
        "# - Replace patterns in the text with special tokens.\n",
        "# - Train the BPE tokenizer on the processed text.\n",
        "# - Encode and decode sample text to verify correctness.\n",
        "\n",
        "# Hints:\n",
        "# - Define special tokens like <IP_ADDR>, <USER_ID>, etc.\n",
        "# - Ensure special tokens are not split during BPE merges.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# Step 6: Data Visualization and Analysis\n",
        "\n",
        "# TODO:\n",
        "# - Analyze the frequency of different log levels.\n",
        "# - Identify the most common actions or errors.\n",
        "# - Create visualizations to represent your findings.\n",
        "\n",
        "# Hints:\n",
        "# - Use pandas for data manipulation.\n",
        "# - Use seaborn or matplotlib for plotting.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# Optional Extensions\n",
        "\n",
        "# TODO:\n",
        "# - Implement anomaly detection.\n",
        "# - Perform temporal analysis.\n",
        "# - Integrate with a machine learning model for classification.\n",
        "\n",
        "# Your code here (if attempting extensions)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "general",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
