{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On 2 - Word Embeddings and Downstream Tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import load_dataset\n",
    "\n",
    "# import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vkutpy-an6uP"
   },
   "source": [
    "## 1. Introduction to Word Embeddings\n",
    "\n",
    "### Why Neural Networks for NLP?\n",
    "\n",
    "Neural networks have revolutionized NLP due to their ability to learn complex patterns and relationships in language data. Here's why they are particularly well-suited for NLP tasks:\n",
    "\n",
    "1. **Handling Sequential Data:** Text is inherently sequential, and neural networks, especially recurrent neural networks (RNNs) and transformers, excel at processing sequential information. They can capture the dependencies and context between words in a sentence or document.\n",
    "2. **Learning Complex Representations:** Word embeddings, generated using neural network-based techniques like Word2Vec and GloVe, capture semantic relationships between words. These representations are far richer than traditional one-hot encodings, allowing neural networks to better understand the meaning of text.\n",
    "3. **Generalization:** Neural networks can generalize well to unseen data, making them robust for a wide range of NLP tasks. They learn underlying linguistic patterns that can be applied to new and diverse text inputs.\n",
    "4. **Adaptability:** Neural network architectures can be adapted and fine-tuned for specific downstream tasks, such as text classification, sentiment analysis, machine translation, and question answering. This flexibility makes them a powerful tool for various NLP applications.\n",
    "5. **Continuous Improvement:** With the availability of large datasets and advancements in deep learning techniques, neural networks continue to improve their performance on NLP tasks, pushing the boundaries of what's possible in natural language understanding.\n",
    "\n",
    "By leveraging the power of neural networks, we can develop sophisticated models that can effectively process, analyze, and generate human language. This has led to significant advancements in various NLP applications, including chatbots, virtual assistants, and text summarization tools.\n",
    "\n",
    "In this hands-on session, we'll explore how neural networks are used to create word embeddings and how these embeddings can be utilized for various downstream tasks.\n",
    "\n",
    "### Why Word Embeddings?\n",
    "In NLP, word embeddings refers to the processo of transforming words into a mathematical format that neural networks can understand. In this hands-on session we will explore the pros and cons of the basic use of neural networks in NLP. This proces, lead the way to the modern LLM and trasformers architectures.\n",
    "<center>\n",
    "    <img src=\"./schema.png\" width=\"1000\" height=\"700\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "### One-hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA5Ox31wkaic"
   },
   "outputs": [],
   "source": [
    "# Sample vocabulary  (obtained from tokenization)\n",
    "vocab = [\n",
    "    \"king\",\n",
    "    \"queen\",\n",
    "    \"man\",\n",
    "    \"woman\",\n",
    "    \"apple\",\n",
    "    \"orange\",\n",
    "    \"banana\",\n",
    "    \"grape\",\n",
    "    \"lion\",\n",
    "    \"tiger\",\n",
    "    \"dog\",\n",
    "    \"cat\",\n",
    "    \"car\",\n",
    "    \"bike\",\n",
    "    \"train\",\n",
    "    \"plane\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "one_hot_encodings = {\n",
    "    word: [1 if i == idx else 0 for i in range(len(vocab))]\n",
    "    for idx, word in enumerate(vocab)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one-hot encodings\n",
    "for word, encoding in one_hot_encodings.items():\n",
    "    print(f\"{word}: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5vov7O2w7HZ"
   },
   "outputs": [],
   "source": [
    "### Exercise: Explore Distances in One-Hot Encoding\n",
    "\n",
    "#  Calculate the Euclidean distance between \"king\" and \"queen\" in the one-hot encoding.\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    return sum((x - y) ** 2 for x, y in zip(vec1, vec2)) ** 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = one_hot_encodings[\"king\"]\n",
    "queen = one_hot_encodings[\"queen\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocab:\n",
    "    print(\n",
    "        f\"Euclidean distance between 'king' and '{word}': {euclidean_distance(king, one_hot_encodings[word])}\"\n",
    "    )\n",
    "\n",
    "# Discuss: Does this distance reflect the relationship between the words?\n",
    "# Yes, the distance between \"king\" and \"queen\" is 1, which is the same as the distance between \"king\" and \"apple\" or \"king\" and \"lion\". This is because the one-hot encoding treats all words as independent and does not capture any relationships between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0o63TAWw4XS"
   },
   "source": [
    "## 2. N-Grams and Contextual Representations\n",
    "### Distributed Representations and Co-Occurrence\n",
    "Word embeddings leverage co-occurrence patterns to create dense, low-dimensional vectors that capture word relationships. This idea, known as Firth's Hypothesis (**\"You shall know a word by the company it keeps\"**__**), forms the foundation of distributed word embeddings. Co-occurence are also referred as n-grams, which are sequences of n words that appear together in a text. The co-occurrence matrix is a square matrix where the rows and columns represent words, and the cell values indicate how often two words appear together in a given context window. By analyzing these co-occurrence patterns, we can learn meaningful representations of words that capture their semantic relationships.\n",
    "\n",
    "N-grams were popularized by the Google N-gram dataset, which contains n-grams extracted from a large corpus of text. These n-grams have been used in various NLP tasks, such as language modeling, machine translation, and sentiment analysis. By analyzing the co-occurrence patterns of words, we can extract valuable insights about their meanings and relationships. \n",
    "\n",
    "__Resources:__\n",
    "\n",
    "- [Google N-gram](https://research.google/blog/all-our-n-gram-are-belong-to-you/)\n",
    "\n",
    "- [The Zipfs Mistery](https://www.youtube.com/watch?v=fCn8zs912OE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5tebjdyxHKi",
    "outputId": "5c831ecf-c6fd-4e5b-90d7-8ffe4ed340bb"
   },
   "outputs": [],
   "source": [
    "# Example corpus\n",
    "corpus = [\"king and queen\", \"man and woman\", \"apple and fruit\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a co-occurrence matrix (simple example, ignoring NLP processing for simplicity)\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "vocab_set = set(word for sentence in corpus for word in sentence.split())\n",
    "vocab_list = list(vocab_set)\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix = np.zeros((len(vocab_list), len(vocab_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill co-occurrence matrix\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(i + 1, len(words)):\n",
    "            co_occurrence_matrix[vocab_dict[word]][vocab_dict[words[j]]] += 1\n",
    "            co_occurrence_matrix[vocab_dict[words[j]]][vocab_dict[word]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary:\", vocab_list)\n",
    "print(\"Co-occurrence Matrix:\\n\", co_occurrence_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5QUhN6pxfTi"
   },
   "outputs": [],
   "source": [
    "### Exercise: Examine the Co-Occurrence Matrix\n",
    "# Choose two words and inspect their co-occurrence counts. Are words with similar contexts close in the matrix?\n",
    "\n",
    "word1 = \"king\"\n",
    "word2 = \"queen\"\n",
    "idx1 = vocab_dict[word1]\n",
    "idx2 = vocab_dict[word2]\n",
    "print(\n",
    "    f\"Co-occurrence count between '{word1}' and '{word2}': {co_occurrence_matrix[idx1][idx2]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = vocab_dict[\"king\"]\n",
    "idx2 = vocab_dict[\"apple\"]\n",
    "\n",
    "print(\n",
    "    f\"Co-occurrence count between 'king' and 'apple': {co_occurrence_matrix[idx1][idx2]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfCsZrd5xhi4"
   },
   "source": [
    "## 3. Dummy Word Embeddings\n",
    "### Word Embeddings with Neural Networks\n",
    "Let's start by creating dummy word embeddings using a simple neural network architecture. We'll use a Word2Vec-like model to learn embeddings for a small vocabulary of words. This model will be trained on a synthetic dataset to demonstrate the process of generating word embeddings using neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me1xvObMxmJs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define vocabulary and toy dataset\n",
    "vocab = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"fruit\"]\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy dataset of (input_word, target_word) pairs\n",
    "data = [(\"king\", \"queen\"), (\"man\", \"woman\"), (\"apple\", \"fruit\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feed-forward neural network\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, word):\n",
    "        return self.embedding(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "embedding_dim = 100\n",
    "model = WordEmbedding(vocab_size, embedding_dim)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(3000):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_idx = torch.tensor([word_to_ix[context]], dtype=torch.long)\n",
    "        target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        context_embedding = model(context_idx)\n",
    "        target_embedding = model(target_idx)\n",
    "\n",
    "        loss = loss_function(context_embedding, target_embedding)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOiobaS3x21B"
   },
   "source": [
    "## 4. Evaluating Word Embeddings\n",
    "The quality of embeddings can be tested by checking if they reflect relationships in the vector space, such as \"king - man + woman â‰ˆ queen\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLVSmi8xx-er"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve embeddings\n",
    "king_vec = model.embedding(torch.tensor(word_to_ix[\"king\"])).detach().numpy()\n",
    "queen_vec = model.embedding(torch.tensor(word_to_ix[\"queen\"])).detach().numpy()\n",
    "# Cosine similarity\n",
    "print(\"Cosine similarity (king, queen):\", cosine_similarity(king_vec, queen_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_vec = model.embedding(torch.tensor(word_to_ix[\"man\"])).detach().numpy()\n",
    "woman_vec = model.embedding(torch.tensor(word_to_ix[\"woman\"])).detach().numpy()\n",
    "print(\"Cosine similarity (man, woman):\", cosine_similarity(man_vec, woman_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine similarity (king, woman):\", cosine_similarity(king_vec, woman_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pre-trained Word2Vec Embeddings\n",
    "Pre-trained embeddings like Word2Vec, GloVe, and FastText are available for various languages and domains. These embeddings capture rich semantic relationships and can be used in downstream tasks without the need for training from scratch.\n",
    "\n",
    "The ground-breaking Word2Vec model, developed by Tomas Mikolov and colleagues at Google was one of the first neural network-based methods for learning word embeddings. Word2Vec uses a shallow neural network to learn word representations from large text corpora. The model is trained to predict the context words given a target word (Skip-gram model) or predict the target word given the context words (Continuous Bag of Words model). The resulting word embeddings capture semantic relationships between words and are widely used in NLP tasks. \n",
    "\n",
    "The model was important because it showed that neural networks could learn meaningful representations of words from raw text data. In addition it was the first model to show the importance of pre-trained embeddings in NLP tasks. Nowdays all the state-of-the-art models are based on pre-trained embeddings.\n",
    "\n",
    "[Word2Vec Paper](https://arxiv.org/abs/1301.3781) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Exploring Pre-trained Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gensim.downloader as api  # For loading pretrained Word2Vec embeddings\n",
    "import nltk  # Natural Language Toolkit (NLTK) for NLP tasks\n",
    "from nltk.corpus import (\n",
    "    stopwords,\n",
    ")  # For removing common words that don't add much meaning\n",
    "from datasets import load_dataset  # Hugging Face library to load datasets\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "from sklearn.decomposition import PCA  # For dimensionality reduction\n",
    "from sklearn.ensemble import RandomForestClassifier  # Model for sentiment analysis\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")  # For evaluating model performance\n",
    "from sklearn.model_selection import train_test_split  # For splitting the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download and set up necessary nltk datasets\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# 1. Load Pretrained Word2Vec Model Using Gensim\n",
    "# Here, we'll use the pretrained 'word2vec-google-news-300' embeddings from Gensim.\n",
    "print(\"Loading pretrained Word2Vec model...\")\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "print(\"Word2Vec model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"happy\"\n",
    "num_similar = 10\n",
    "\n",
    "# Find words most similar to the given word based on cosine similarity\n",
    "similar_words = word2vec_model.most_similar(word, topn=num_similar)\n",
    "similar_words = [(word, 1.0)] + similar_words  # Include the original word itself\n",
    "\n",
    "# Collect embeddings of the word and its most similar words\n",
    "embeddings = np.array([word2vec_model[w[0]] for w in similar_words])\n",
    "words = [w[0] for w in similar_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot the embeddings in 2D space\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], marker=\"o\", color=\"blue\")\n",
    "\n",
    "# Annotate each point with the corresponding word\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=12)\n",
    "\n",
    "plt.title(f\"2D Visualization of '{word}' and its Most Similar Words\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore a very famous property of word2vec embeddings: the linear relationships between words. The linear rappresentation means that the words can be represented as a linear combination of other words. In other words we can see that:\n",
    "$$\n",
    "\\mathrm{embedding(\\text{king})} + \\mathrm{embedding(\\text{woman})} - \\mathrm{embedding(\\text{man})} = \\mathrm{embedding(\\text{queen})} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the linear relationship between word embeddings (king + woman - man = queen)\n",
    "queen_vec = word2vec_model[\"queen\"]\n",
    "king_vec = word2vec_model[\"king\"]\n",
    "man_vec = word2vec_model[\"man\"]\n",
    "woman_vec = word2vec_model[\"woman\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_queen = king_vec + woman_vec - man_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between the expected and calculated vectors\n",
    "cosine_similarity_queen = cosine_similarity(queen_vec, linear_queen)\n",
    "print(\"Cosine similarity (queen, king):\", cosine_similarity_queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Using Pre-trained Word2Vec Embeddings in Downstream Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sentiment Analysis Using Word Embeddings and IMDB Dataset from Hugging Face\n",
    "# We'll use the IMDB movie reviews dataset provided by Hugging Face, which contains 50,000 movie reviews labeled\n",
    "# as either 'positive' or 'negative'.\n",
    "\n",
    "# Load the IMDB dataset from Hugging Face\n",
    "print(\"Loading IMDB dataset...\")\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "print(\"IMDB dataset loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset: tokenize reviews, remove stop words, and average word vectors to get sentence embeddings\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_review(review):\n",
    "    \"\"\"\n",
    "    Convert a review to a single vector by averaging the Word2Vec embeddings of the words in the review.\n",
    "\n",
    "    Parameters:\n",
    "    review (str): The review text.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The averaged Word2Vec vector for the review.\n",
    "    \"\"\"\n",
    "    # Tokenize the review into words and filter out stopwords\n",
    "    words = nltk.word_tokenize(review.lower())\n",
    "    word_vectors = [\n",
    "        word2vec_model[word]\n",
    "        for word in words\n",
    "        if word in word2vec_model and word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Return the mean of all word vectors in the review to get a single vector representation for the review\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # If no valid word vectors are found, return a zero vector (300-dimensional)\n",
    "        return np.zeros(word2vec_model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset by extracting features and labels\n",
    "def prepare_data(dataset):\n",
    "    \"\"\"\n",
    "    Prepare the dataset by converting reviews to word vector averages and extracting labels.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (Dataset): Hugging Face Dataset containing IMDB reviews.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray, np.ndarray: Arrays of review vectors and labels.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    labels = []\n",
    "\n",
    "    for item in dataset:\n",
    "        # Preprocess each review and add to the list\n",
    "        review_vector = preprocess_review(item[\"text\"])\n",
    "        reviews.append(review_vector)\n",
    "        labels.append(1 if item[\"label\"] == 1 else 0)  # 1 for positive, 0 for negative\n",
    "\n",
    "    return np.array(reviews), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets and prepare data\n",
    "train_data = imdb_dataset[\"train\"]\n",
    "test_data = imdb_dataset[\"test\"]\n",
    "\n",
    "X_train, y_train = prepare_data(train_data)\n",
    "X_test, y_test = prepare_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier: Random Forest Classifier\n",
    "print(\"\\nTraining sentiment analysis model...\")\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "print(\"Model trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print a classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement your own Word2Vec model\n",
    "\n",
    "**!! Disable Co-Pilot for this section !!**\n",
    "\n",
    "Now it's time to implement your own Word2Vec model. The needed steps are:\n",
    "1. Data Preprocessing: First, we need to preprocess the text data by tokenizing the sentences and creating a vocabulary. \n",
    "2. Data Generation: Next, we generate training data for the Word2Vec model using the skip-gram approach.\n",
    "3. Model Architecture: We define a neural network architecture to learn word embeddings from the training data.\n",
    "4. Training: We train the Word2Vec model using the training data and optimize it using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's load the data\n",
    "data = load_dataset(\"Daniele/dante-corpus\", split=\"train[:100]\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preprocessing\n",
    "Let's start by preprocessing the text input to create a vocabulary. You have to implement the following steps:\n",
    "- Tokenize the sentences into words.\n",
    "- Populate a vocabulary with unique words from the text data.\n",
    "- Assign an index to each word in the vocabulary.\n",
    "- Store all the tokenized sentences as a list of lists of word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}  # Dictionary to convert words to indices for example {'king': 0, 'queen': 1, ...}\n",
    "ix_to_word = {}  # Dictionary to convert indices to words for example {0: 'king', 1: 'queen', ...}\n",
    "tokenized_dataset = []  # List to store the tokenized\n",
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "print(\"Vocabulary size:\", len(word_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to generate the dataset for training. The dataset should be a list of tuples where each tuple is a pair of the target word and the context word. The context words are the words that appear in a window of size 2 around the target word. So if the sentence is __\"the quick brown fox jumps over the lazy dog\"__ and the target word is \"fox\" the context words are [\"quick\", \"brown\", \"jumps\", \"over\"]. Other example are:\n",
    "\n",
    "<center>\n",
    "    <img src=\"./context_window.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_window = 2\n",
    "\n",
    "train_dataset = []\n",
    "# your code here\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the function to map any word to one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the words\n",
    "# You can choose to one-hot encode words.\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "\n",
    "def one_hot_encoding(word: str) -> torch.Tensor:\n",
    "    pass  # your code here\n",
    "\n",
    "\n",
    "def one_hot_decoding(one_hot: torch.Tensor) -> str:\n",
    "    pass  # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to implement the model. The model is a very simple neural network with two linear layers. The input is the one-hot encoding of the target word and the output is the one-hot encoding of the context word:\n",
    "<center>\n",
    "    <img src=\"./word2vec.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    pass\n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the loss function. We can use\n",
    "1. Softmax\n",
    "2. CrossEntropyLoss\n",
    "3. Noise Contrastive Estimation (NCE) Loss\n",
    "4. Negative Sampling (in the original Word2Vec paper)\n",
    "\n",
    "For now, we will use just use the CrossEntropyLoss. If you want to try the other loss functions, you can check this [blog](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer and the model\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model = SkipGramModel(vocab_size, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the training loop. The training loop should:\n",
    "1. Get the target and context words from the dataset\n",
    "2. Get the one-hot encoding of the target word\n",
    "3. Get the one-hot encoding of the context word\n",
    "4. Fed the target word to the model\n",
    "5. Compute the loss\n",
    "6. Optimize the model using backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# train the model\n",
    "epochs = 300\n",
    "losses = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
