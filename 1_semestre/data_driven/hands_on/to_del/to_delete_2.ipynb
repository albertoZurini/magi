{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from control import lqr, ss\n",
    "from control.matlab import lsim\n",
    "import random\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1wTfx0woowE"
   },
   "source": [
    "**The Tabular value-fucntion methods on the cart and pole system**\n",
    "\n",
    "Consider the cart-and-pole system here depicted:\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Yu5zZ57OEEYZ678yWLaU7h-Wq3M9vV9E\" width=\"200\">\n",
    "</div>\n",
    "whose dynamics follows the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "  \\ddot{\\theta}&=\\frac{g \\sin(\\theta)\\,+\\,\\cos(\\theta) \\left[ \\frac{-F\\,-\\,m_p\\,l\\,\\dot{\\theta}^2 \\sin \\left(\\theta\\right)}{m_c\\,+\\,m_p}\\right]-\\frac{\\mu_p\\dot{\\theta}}{m_p\\,l}}{l\\,\\left[\\frac{4}{3}-\\frac{m_p\\,cos^2\\left(\\theta\\right)}{m_c\\,+\\,m_p}\\right]}\n",
    "\\end{align}\n",
    ">>>>>>>>>>>>>>>>>>>>>>>>$(*)$\n",
    "\n",
    "\\begin{align}\n",
    "  \\ddot{p}_c&=\\frac{F\\,+\\,m_p\\,l\\,\\left[\\dot{\\theta}^2\\,\\sin\\left(\\theta\\right)\\,-\\,\\ddot{\\theta}\\,\\cos(\\theta)\\right]}{m_c\\,+\\,m_p}\n",
    "\\end{align}\n",
    "\n",
    "where $g = 9.8\\,\\text{m/sec}$ is the gravitational acceleration, $m_c = 1\\,\\text{kg}$ is the cart mass, $m_p = 0.1\\,\\text{kg}$ is the pole mass, $l = 0.5\\,\\text{m}$ is the half-pole length, $\\mu_p = 0.000002$ is the pole on cart friction coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7Wi1yiC6vfk"
   },
   "source": [
    "1. Given the $\\mathcal{X}$, $\\mathcal{U}$ of the first hands-on define: the set of possible initial conditions of an episode; the terminal conditions of the episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eWnnT1HOpD2"
   },
   "source": [
    "The set of all possibile initial conditions is:\n",
    "\n",
    "$$\n",
    "x = \\left( \\begin{array}{ccc}\n",
    "                \\theta \\\\\n",
    "                \\dot \\theta \\\\\n",
    "                p_c \\\\\n",
    "                \\dot p_c\n",
    "\\end{array} \\right)\n",
    "=\n",
    "\\left( \\begin{array}{ccc}\n",
    "                  x_1 \\\\\n",
    "                  x_2 \\\\\n",
    "                  x_3 \\\\\n",
    "                  x_4\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "And the terminal episodes are:\n",
    "1. if the pole felt, $\\theta = \\{-\\frac{\\pi}{4}; \\frac{\\pi}{4}\\} rad$\n",
    "2. if the cart is going out of the constraints $|p_c| > 5m$\n",
    "3. if the control input force is too high $|u| > 10N$\n",
    "4. If the episode is taking more than $10s$ to complete (500 time steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM2Bv54WOwAP"
   },
   "source": [
    "2. Provide a proper discretization of the state space $\\mathcal{X}$, and define the size of the action-value function $Q$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ags1D7QKRGbd"
   },
   "source": [
    "3. Create a code able to select the discretized version of a measured state $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVeTz-WESwO2"
   },
   "source": [
    "4.    Given the terminal conditions of the episode (selected in 1.) create a code able to perform a fixed number of episodes of a fixed number of steps, starting from random initial conditions and applying random inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsgvNfmxTc1z"
   },
   "source": [
    "5.   Create a function able to perform $\\epsilon$-greedy policy for a chosen $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mojNOsTCTBxe"
   },
   "source": [
    "6. Create a code that applies Tabular Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible inputs are either $F=10$ or $F=-10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. discretizza in piu range possibili\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cart_and_pole_odeint(x, t, F):\n",
    "  dxdt = np.zeros_like(x)\n",
    "\n",
    "  dxdt[0] = x[1] #tetap\n",
    "  dxdt[1] = (g*sin(x[0])+ cos(x[0])*((-F-m*l*(x[1]**2)*sin(x[0]))/(mc+m))-((miup*x[1])/(m*l))) / (l*((4/3)-((m*(cos(x[0])**2))/(mc+m))))    #teta2p\n",
    "  dxdt[2] = x[3] #xp\n",
    "  dxdt[3] = (F+m*l*((x[1]**2)*sin(x[0])-dxdt[1]*cos(x[0])))/(mc+m) #x2p\n",
    "  return dxdt\n",
    "\n",
    "g = 9.8 # gravitational acceleration\n",
    "mc = 1  # cart mass [kg]\n",
    "l = 0.5 # half-pole length [m]\n",
    "m = 0.1 # pole mass [kg]\n",
    "miup = 2e-6 # pole friction coefficient\n",
    "\n",
    "time_step=0.02\n",
    "t0_odeint=0\n",
    "x0_odeint = np.array([np.deg2rad(1), 0.0, 0.0, 0.0]) # start out of equilibrium\n",
    "X_odeint=[x0_odeint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowerBounds = [\n",
    "    deg2rad(-25),\n",
    "    -1,\n",
    "    -5,\n",
    "    -2\n",
    "]\n",
    "upperBounds = [- i for i in lowerBounds]\n",
    "numberOfBins = [\n",
    "    10,\n",
    "    10,\n",
    "    10,\n",
    "    10\n",
    "]\n",
    "\n",
    "def discretize_state(state):\n",
    "    angle    =      state[0]\n",
    "    angularVelocity=state[1]\n",
    "    position =      state[2]\n",
    "    velocity =      state[3]\n",
    "\n",
    "    poleAngleBin=np.linspace(lowerBounds[0],upperBounds[0],numberOfBins[0])\n",
    "    poleAngleVelocityBin=np.linspace(lowerBounds[1],upperBounds[1],numberOfBins[1])\n",
    "    cartPositionBin=np.linspace(lowerBounds[2],upperBounds[2],numberOfBins[2])\n",
    "    cartVelocityBin=np.linspace(lowerBounds[3],upperBounds[3],numberOfBins[3])\n",
    "\n",
    "    indexAngle=np.maximum(np.digitize(angle,poleAngleBin)-1,0)\n",
    "    indexAngularVelocity=np.maximum(np.digitize(angularVelocity,poleAngleVelocityBin)-1,0)\n",
    "    indexPosition=np.maximum(np.digitize(position,cartPositionBin)-1,0)\n",
    "    indexVelocity=np.maximum(np.digitize(velocity,cartVelocityBin)-1,0)\n",
    "\n",
    "    return tuple([indexAngle,indexAngularVelocity,indexPosition,indexVelocity])\n",
    "\n",
    "def is_terminal_state(dstate):\n",
    "    # discretized state\n",
    "    if dstate[0] == 0 or dstate[0] == numberOfBins[0]-1:\n",
    "        return True\n",
    "    if dstate[2] == 0 or dstate[2] == numberOfBins[2]-1:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_episodes = 1000\n",
    "max_steps= 500\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.001             # Exponential decay rate for exploration prob\n",
    "\n",
    "action_size = 2 # left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = np.zeros(tuple(numberOfBins) + (action_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Episode 12'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m t1_odeint \u001b[38;5;241m=\u001b[39m t0_odeint\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.02\u001b[39m\n\u001b[1;32m     37\u001b[0m t_odeint \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([t0_odeint, t1_odeint])\n\u001b[0;32m---> 38\u001b[0m x_odeint \u001b[38;5;241m=\u001b[39m odeint(cart_and_pole_odeint, state_c, t_odeint, args\u001b[38;5;241m=\u001b[39m(F, ))\n\u001b[1;32m     39\u001b[0m new_state_c \u001b[38;5;241m=\u001b[39m x_odeint[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     40\u001b[0m new_state \u001b[38;5;241m=\u001b[39m discretize_state(new_state_c)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/integrate/_odepack_py.py:242\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, args, Dfun, col_deriv, full_output, ml, mu, rtol, atol, tcrit, h0, hmax, hmin, ixpr, mxstep, mxhnil, mxordn, mxords, printmessg, tfirst)\u001b[0m\n\u001b[1;32m    240\u001b[0m t \u001b[38;5;241m=\u001b[39m copy(t)\n\u001b[1;32m    241\u001b[0m y0 \u001b[38;5;241m=\u001b[39m copy(y0)\n\u001b[0;32m--> 242\u001b[0m output \u001b[38;5;241m=\u001b[39m _odepack\u001b[38;5;241m.\u001b[39modeint(func, y0, t, args, Dfun, col_deriv, ml, mu,\n\u001b[1;32m    243\u001b[0m                          full_output, rtol, atol, tcrit, h0, hmax, hmin,\n\u001b[1;32m    244\u001b[0m                          ixpr, mxstep, mxhnil, mxordn, mxords,\n\u001b[1;32m    245\u001b[0m                          \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mbool\u001b[39m(tfirst)))\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    247\u001b[0m     warning_msg \u001b[38;5;241m=\u001b[39m _msgs[output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Run with full_output = 1 to get quantitative information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mcart_and_pole_odeint\u001b[0;34m(x, t, F)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcart_and_pole_odeint\u001b[39m(x, t, F):\n\u001b[0;32m----> 2\u001b[0m   dxdt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(x)\n\u001b[1;32m      4\u001b[0m   dxdt[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#tetap\u001b[39;00m\n\u001b[1;32m      5\u001b[0m   dxdt[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m (g\u001b[38;5;241m*\u001b[39msin(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m cos(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m((\u001b[38;5;241m-\u001b[39mF\u001b[38;5;241m-\u001b[39mm\u001b[38;5;241m*\u001b[39ml\u001b[38;5;241m*\u001b[39m(x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39msin(x[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m/\u001b[39m(mc\u001b[38;5;241m+\u001b[39mm))\u001b[38;5;241m-\u001b[39m((miup\u001b[38;5;241m*\u001b[39mx[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m/\u001b[39m(m\u001b[38;5;241m*\u001b[39ml))) \u001b[38;5;241m/\u001b[39m (l\u001b[38;5;241m*\u001b[39m((\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m-\u001b[39m((m\u001b[38;5;241m*\u001b[39m(cos(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m/\u001b[39m(mc\u001b[38;5;241m+\u001b[39mm))))    \u001b[38;5;66;03m#teta2p\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mzeros_like\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/numeric.py:141\u001b[0m, in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[1;32m    139\u001b[0m res \u001b[38;5;241m=\u001b[39m empty_like(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, subok\u001b[38;5;241m=\u001b[39msubok, shape\u001b[38;5;241m=\u001b[39mshape)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# needed instead of a 0 to get same result as zeros for string dtypes\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m z \u001b[38;5;241m=\u001b[39m zeros(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    142\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(res, z, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    clear_output(wait=True)\n",
    "    display('Episode '+str(episode))\n",
    "\n",
    "    # Reset the environment\n",
    "    state_c = x0_odeint\n",
    "    state = discretize_state(state_c)\n",
    "    step = 0\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = random.randint(0,1)\n",
    "            \n",
    "        # convert action into force\n",
    "        if action == 1:\n",
    "            F = 10\n",
    "        else:\n",
    "            F = -10\n",
    "\n",
    "        # Simulate the system\n",
    "        # new_state, reward, done, info, _ = env.step(action)\n",
    "        t1_odeint = t0_odeint+0.02\n",
    "        t_odeint = np.array([t0_odeint, t1_odeint])\n",
    "        x_odeint = odeint(cart_and_pole_odeint, state_c, t_odeint, args=(F, ))\n",
    "        new_state_c = x_odeint[1]\n",
    "        new_state = discretize_state(new_state_c)\n",
    "        \n",
    "        if is_terminal_state(new_state):\n",
    "            reward = -100\n",
    "        else:\n",
    "            reward = 1\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state + (action,)] = qtable[state + (action,)] + learning_rate * (reward + gamma * np.max(qtable[new_state]) - qtable[state + (action,)])\n",
    "        \n",
    "        total_rewards =total_rewards + reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        state_c = new_state_c\n",
    "\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if is_terminal_state(new_state):\n",
    "            break\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "#print(qtable)\n",
    "#print(epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards, 'r', label='U(t)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_odeint = [x0_odeint]\n",
    "uvec = []\n",
    "\n",
    "state_c = x0_odeint\n",
    "state = discretize_state(state_c)\n",
    "total_rewards = 0\n",
    "\n",
    "for step in range(max_steps):\n",
    "    action = np.argmax(qtable[state])\n",
    "\n",
    "    # convert action into force\n",
    "    if action == 1:\n",
    "        F = 10\n",
    "    else:\n",
    "        F = -10\n",
    "        \n",
    "    uvec.append(F)\n",
    "\n",
    "    # Simulate the system\n",
    "    # new_state, reward, done, info, _ = env.step(action)\n",
    "    t1_odeint = t0_odeint+0.02\n",
    "    t_odeint = np.array([t0_odeint, t1_odeint])\n",
    "    x_odeint = odeint(cart_and_pole_odeint, state_c, t_odeint, args=(F, ))\n",
    "    new_state_c = x_odeint[1]\n",
    "    new_state = discretize_state(new_state_c)\n",
    "    X_odeint=np.append(X_odeint, [state_c], axis= 0)\n",
    "\n",
    "    if is_terminal_state(new_state):\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = 1\n",
    "\n",
    "    # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "    # qtable[new_state,:] : all the actions we can take from new state\n",
    "    # qtable[state + (action,)] = qtable[state + (action,)] + learning_rate * (reward + gamma * np.max(qtable[new_state]) - qtable[state + (action,)])\n",
    "\n",
    "    total_rewards =total_rewards + reward\n",
    "\n",
    "    # Our new state is state\n",
    "    state = new_state\n",
    "    state_c = new_state_c\n",
    "\n",
    "    # If done (if we're dead) : finish episode\n",
    "    if is_terminal_state(new_state):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Time:\", np.arange(0.0, 10, time_step)[:len(uvec)][-1])\n",
    "time_sim=np.arange(0.0, 10+time_step, time_step)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 15]\n",
    "fig, axs = plt.subplots(5)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "axs[0].plot(np.arange(0.0, 10, time_step)[:len(uvec)], uvec, 'r', label='U(t)')\n",
    "axs[0].set_title(\"U\")\n",
    "\n",
    "axs[1].plot(time_sim[:X_odeint[:,0].shape[0]], X_odeint[:,0], 'b')\n",
    "axs[1].set_title(\"theta(t)\")\n",
    "\n",
    "axs[2].plot(time_sim[:X_odeint[:,1].shape[0]], X_odeint[:,1], 'b')\n",
    "axs[2].set_title(\"dot theta(t)\")\n",
    "\n",
    "axs[3].plot(time_sim[:X_odeint[:,2].shape[0]], X_odeint[:,2], 'b')\n",
    "axs[3].set_title(\"p_c(t)\")\n",
    "\n",
    "axs[4].plot(time_sim[:X_odeint[:,3].shape[0]], X_odeint[:,3], 'b')\n",
    "axs[4].set_title(\"dot p_c(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TgpbbNqf4Ks"
   },
   "source": [
    "7.    Copy and paste your Q-learning algorithm and apply the changes needed to convert it into a SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qtable_sarsa = np.zeros(tuple(numberOfBins) + (action_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of rewards\n",
    "desired_state_c = [0,0,0,0]\n",
    "desired_state = discretize_state(desired_state_c)\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    clear_output(wait=True)\n",
    "    display('Episode '+str(episode))\n",
    "\n",
    "    # Reset the environment\n",
    "    state_c = x0_odeint\n",
    "    state = discretize_state(state_c)\n",
    "    step = 0\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable_sarsa[state])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = random.randint(0,1)\n",
    "            \n",
    "        # convert action into force\n",
    "        if action == 1:\n",
    "            F = 10\n",
    "        else:\n",
    "            F = -10\n",
    "\n",
    "        # Simulate the system\n",
    "        t1_odeint = t0_odeint+0.02\n",
    "        t_odeint = np.array([t0_odeint, t1_odeint])\n",
    "        x_odeint = odeint(cart_and_pole_odeint, state_c, t_odeint, args=(F, ))\n",
    "        new_state_c = x_odeint[1]\n",
    "        new_state = discretize_state(new_state_c)\n",
    "        \n",
    "        if is_terminal_state(new_state):\n",
    "            reward = -100\n",
    "        else:\n",
    "            reward = 1\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable_sarsa[state + (action,)] = qtable_sarsa[state + (action,)] + learning_rate * (reward + gamma * qtable_sarsa[new_state + (action,)] - qtable_sarsa[desired_state + (action,)])\n",
    "        \n",
    "        total_rewards =total_rewards + reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        state_c = new_state_c\n",
    "\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if is_terminal_state(new_state):\n",
    "            break\n",
    "        \n",
    "    episode += 1\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "#print(qtable)\n",
    "#print(epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(rewards, 'r', label='U(t)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the q matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qtable.npy\", \"wb\") as f:\n",
    "    np.save(f, qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"qtable_sarsa.npy\", \"wb\") as f:\n",
    "    np.save(f, qtable_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_odeint = [x0_odeint]\n",
    "uvec = []\n",
    "\n",
    "state_c = x0_odeint\n",
    "state = discretize_state(state_c)\n",
    "total_rewards = 0\n",
    "\n",
    "for step in range(max_steps):\n",
    "    action = np.argmax(qtable_sarsa[state])\n",
    "\n",
    "    # convert action into force\n",
    "    if action == 1:\n",
    "        F = 10\n",
    "    else:\n",
    "        F = -10\n",
    "        \n",
    "    uvec.append(F)\n",
    "\n",
    "    # Simulate the system\n",
    "    # new_state, reward, done, info, _ = env.step(action)\n",
    "    t1_odeint = t0_odeint+0.02\n",
    "    t_odeint = np.array([t0_odeint, t1_odeint])\n",
    "    x_odeint = odeint(cart_and_pole_odeint, state_c, t_odeint, args=(F, ))\n",
    "    new_state_c = x_odeint[1]\n",
    "    new_state = discretize_state(new_state_c)\n",
    "    X_odeint=np.append(X_odeint, [state_c], axis= 0)\n",
    "\n",
    "    if is_terminal_state(new_state):\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = 1\n",
    "\n",
    "    total_rewards =total_rewards + reward\n",
    "\n",
    "    # Our new state is state\n",
    "    state = new_state\n",
    "    state_c = new_state_c\n",
    "\n",
    "    # If done (if we're dead) : finish episode\n",
    "    if is_terminal_state(new_state):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Time:\", np.arange(0.0, 10, time_step)[:len(uvec)][-1])\n",
    "time_sim=np.arange(0.0, 10+time_step, time_step)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 15]\n",
    "fig, axs = plt.subplots(5)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "axs[0].plot(np.arange(0.0, 10, time_step)[:len(uvec)], uvec, 'r', label='U(t)')\n",
    "axs[0].set_title(\"U\")\n",
    "\n",
    "axs[1].plot(time_sim[:X_odeint[:,0].shape[0]], X_odeint[:,0], 'b')\n",
    "axs[1].set_title(\"theta(t)\")\n",
    "\n",
    "axs[2].plot(time_sim[:X_odeint[:,1].shape[0]], X_odeint[:,1], 'b')\n",
    "axs[2].set_title(\"dot theta(t)\")\n",
    "\n",
    "axs[3].plot(time_sim[:X_odeint[:,2].shape[0]], X_odeint[:,2], 'b')\n",
    "axs[3].set_title(\"p_c(t)\")\n",
    "\n",
    "axs[4].plot(time_sim[:X_odeint[:,3].shape[0]], X_odeint[:,3], 'b')\n",
    "axs[4].set_title(\"dot p_c(t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
